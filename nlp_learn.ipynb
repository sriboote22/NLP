{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "#nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"John had always been fascinated by technology. Ever since he was a child, he dreamed of becoming a software engineer. After years of studying and hard work, he finally landed a job at a well-known tech company, TechCorp. His first project was to develop an AI-based system that could analyze customer feedback and provide real-time insights. Excited and nervous at the same time, John knew this was the opportunity he had been waiting for.\"\n",
    "\n",
    "\"One day, while taking a break, John met Lisa, a data scientist who specialized in machine learning. They quickly bonded over their shared passion for AI and robotics. Together, they brainstormed innovative ideas that could revolutionize the company's products. Their combined efforts soon paid off, as their AI system became one of the company's top-performing tools.\"\n",
    "\n",
    "\"As months passed, John found himself thinking more about starting his own tech startup. He knew the risks involved but felt that the potential rewards far outweighed them. With Lisa's encouragement and the support of a few close colleagues, John decided to take the leap and begin his entrepreneurial journey.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John had always been fascinated by technology.', 'Ever since he was a child, he dreamed of becoming a software engineer.', 'After years of studying and hard work, he finally landed a job at a well-known tech company, TechCorp.', 'His first project was to develop an AI-based system that could analyze customer feedback and provide real-time insights.', 'Excited and nervous at the same time, John knew this was the opportunity he had been waiting for.\"', '\"One day, while taking a break, John met Lisa, a data scientist who specialized in machine learning.', 'They quickly bonded over their shared passion for AI and robotics.', \"Together, they brainstormed innovative ideas that could revolutionize the company's products.\", 'Their combined efforts soon paid off, as their AI system became one of the company\\'s top-performing tools.\"', '\"As months passed, John found himself thinking more about starting his own tech startup.', 'He knew the risks involved but felt that the potential rewards far outweighed them.', \"With Lisa's encouragement and the support of a few close colleagues, John decided to take the leap and begin his entrepreneurial journey.\"]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(sentences)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'had', 'always', 'been', 'fascinated', 'by', 'technology', '.', 'Ever', 'since', 'he', 'was', 'a', 'child', ',', 'he', 'dreamed', 'of', 'becoming', 'a', 'software', 'engineer', '.', 'After', 'years', 'of', 'studying', 'and', 'hard', 'work', ',', 'he', 'finally', 'landed', 'a', 'job', 'at', 'a', 'well-known', 'tech', 'company', ',', 'TechCorp', '.', 'His', 'first', 'project', 'was', 'to', 'develop', 'an', 'AI-based', 'system', 'that', 'could', 'analyze', 'customer', 'feedback', 'and', 'provide', 'real-time', 'insights', '.', 'Excited', 'and', 'nervous', 'at', 'the', 'same', 'time', ',', 'John', 'knew', 'this', 'was', 'the', 'opportunity', 'he', 'had', 'been', 'waiting', 'for', '.', \"''\", '``', 'One', 'day', ',', 'while', 'taking', 'a', 'break', ',', 'John', 'met', 'Lisa', ',', 'a', 'data', 'scientist', 'who', 'specialized', 'in', 'machine', 'learning', '.', 'They', 'quickly', 'bonded', 'over', 'their', 'shared', 'passion', 'for', 'AI', 'and', 'robotics', '.', 'Together', ',', 'they', 'brainstormed', 'innovative', 'ideas', 'that', 'could', 'revolutionize', 'the', 'company', \"'s\", 'products', '.', 'Their', 'combined', 'efforts', 'soon', 'paid', 'off', ',', 'as', 'their', 'AI', 'system', 'became', 'one', 'of', 'the', 'company', \"'s\", 'top-performing', 'tools', '.', \"''\", '``', 'As', 'months', 'passed', ',', 'John', 'found', 'himself', 'thinking', 'more', 'about', 'starting', 'his', 'own', 'tech', 'startup', '.', 'He', 'knew', 'the', 'risks', 'involved', 'but', 'felt', 'that', 'the', 'potential', 'rewards', 'far', 'outweighed', 'them', '.', 'With', 'Lisa', \"'s\", 'encouragement', 'and', 'the', 'support', 'of', 'a', 'few', 'close', 'colleagues', ',', 'John', 'decided', 'to', 'take', 'the', 'leap', 'and', 'begin', 'his', 'entrepreneurial', 'journey', '.']\n",
      "210\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(paragraph)\n",
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmimng example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john alway fascin technolog .', 'ever sinc child , dream becom softwar engin .', 'after year studi hard work , final land job well-known tech compani , techcorp .', 'hi first project develop ai-bas system could analyz custom feedback provid real-tim insight .', \"excit nervou time , john knew opportun wait . ''\", '`` one day , take break , john met lisa , data scientist special machin learn .', 'they quickli bond share passion ai robot .', \"togeth , brainstorm innov idea could revolution compani 's product .\", \"their combin effort soon paid , ai system becam one compani 's top-perform tool . ''\", '`` as month pass , john found think start tech startup .', 'he knew risk involv felt potenti reward far outweigh .', \"with lisa 's encourag support close colleagu , john decid take leap begin entrepreneuri journey .\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John always fascinated technology .', 'Ever since child , dreamed becoming software engineer .', 'After year studying hard work , finally landed job well-known tech company , TechCorp .', 'His first project develop AI-based system could analyze customer feedback provide real-time insight .', \"Excited nervous time , John knew opportunity waiting . ''\", '`` One day , taking break , John met Lisa , data scientist specialized machine learning .', 'They quickly bonded shared passion AI robotics .', \"Together , brainstormed innovative idea could revolutionize company 's product .\", \"Their combined effort soon paid , AI system became one company 's top-performing tool . ''\", '`` As month passed , John found thinking starting tech startup .', 'He knew risk involved felt potential reward far outweighed .', \"With Lisa 's encouragement support close colleague , John decided take leap begin entrepreneurial journey .\"]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer                                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "new_sentences = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lisa encouragement support close colleague john decided take leap begin entrepreneurial journey\n",
      "['john always fascinated technology', 'ever since child dreamed becoming software engineer', 'year studying hard work finally landed job well known tech company techcorp', 'first project develop ai based system could analyze customer feedback provide real time insight', 'excited nervous time john knew opportunity waiting', 'one day taking break john met lisa data scientist specialized machine learning', 'quickly bonded shared passion ai robotics', 'together brainstormed innovative idea could revolutionize company product', 'combined effort soon paid ai system became one company top performing tool', 'month passed john found thinking starting tech startup', 'knew risk involved felt potential reward far outweighed', 'lisa encouragement support close colleague john decided take leap begin entrepreneurial journey', 'john always fascinated technology', 'ever since child dreamed becoming software engineer', 'year studying hard work finally landed job well known tech company techcorp', 'first project develop ai based system could analyze customer feedback provide real time insight', 'excited nervous time john knew opportunity waiting', 'one day taking break john met lisa data scientist specialized machine learning', 'quickly bonded shared passion ai robotics', 'together brainstormed innovative idea could revolutionize company product', 'combined effort soon paid ai system became one company top performing tool', 'month passed john found thinking starting tech startup', 'knew risk involved felt potential reward far outweighed', 'lisa encouragement support close colleague john decided take leap begin entrepreneurial journey']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]',' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review =' '.join(review)\n",
    "    new_sentences.append(review)\n",
    "\n",
    "\n",
    "#print(review)\n",
    "print(new_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 1 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv= CountVectorizer()\n",
    "x= cv.fit_transform(new_sentences).toarray()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF -IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.54712758 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.29840749 0.29840749 0.29840749]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf=TfidfVectorizer()\n",
    "y = tf.fit_transform(new_sentences).toarray()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot Encoded Representation: [[17, 5, 15, 8], [17, 5, 15, 1], [17, 12, 15, 3], [13, 11, 10, 14, 7], [13, 11, 10, 14, 16], [2, 17, 9, 15, 0], [4, 6, 18, 14]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sriparna\\AppData\\Local\\Temp\\ipykernel_15164\\3808972963.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  padded_docs = torch.tensor(padded_docs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Sequences:\n",
      " tensor([[ 0,  0,  0,  0, 17,  5, 15,  8],\n",
      "        [ 0,  0,  0,  0, 17,  5, 15,  1],\n",
      "        [ 0,  0,  0,  0, 17, 12, 15,  3],\n",
      "        [ 0,  0,  0, 13, 11, 10, 14,  7],\n",
      "        [ 0,  0,  0, 13, 11, 10, 14, 16],\n",
      "        [ 0,  0,  0,  2, 17,  9, 15,  0],\n",
      "        [ 0,  0,  0,  0,  4,  6, 18, 14]], dtype=torch.int32)\n",
      "EmbeddingModel(\n",
      "  (embedding): Embedding(10000, 10)\n",
      ")\n",
      "Embedded Output:\n",
      " tensor([[[-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-0.4542,  0.1029, -0.3196, -1.7591,  0.9019, -0.7763,  0.4036,\n",
      "           0.2324, -1.3783, -1.2706],\n",
      "         [ 0.7639, -0.7502, -0.5962, -0.0562, -1.0859,  0.4220,  0.2182,\n",
      "          -0.9370, -0.0715, -1.0530],\n",
      "         [ 0.1842,  1.1685,  0.5233, -0.7669,  0.6694, -0.8667,  0.0460,\n",
      "           1.8485,  0.5150, -1.0578],\n",
      "         [-2.4402, -0.3007, -1.3439, -1.7787,  0.0213, -0.6816,  0.7470,\n",
      "          -0.9966,  1.0203, -1.5491]],\n",
      "\n",
      "        [[-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-0.4542,  0.1029, -0.3196, -1.7591,  0.9019, -0.7763,  0.4036,\n",
      "           0.2324, -1.3783, -1.2706],\n",
      "         [ 0.7639, -0.7502, -0.5962, -0.0562, -1.0859,  0.4220,  0.2182,\n",
      "          -0.9370, -0.0715, -1.0530],\n",
      "         [ 0.1842,  1.1685,  0.5233, -0.7669,  0.6694, -0.8667,  0.0460,\n",
      "           1.8485,  0.5150, -1.0578],\n",
      "         [-1.3521, -0.1288,  1.1744,  1.5486,  1.6062, -0.4923, -1.2715,\n",
      "          -0.8308,  0.9646,  1.6918]],\n",
      "\n",
      "        [[-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-0.4542,  0.1029, -0.3196, -1.7591,  0.9019, -0.7763,  0.4036,\n",
      "           0.2324, -1.3783, -1.2706],\n",
      "         [-1.0224,  1.3556,  1.0791, -1.6182,  0.0091,  0.3264,  0.4918,\n",
      "           0.4187,  0.8957, -0.1340],\n",
      "         [ 0.1842,  1.1685,  0.5233, -0.7669,  0.6694, -0.8667,  0.0460,\n",
      "           1.8485,  0.5150, -1.0578],\n",
      "         [ 0.9547, -0.0598, -1.2429,  0.5386, -2.0858,  0.3448, -0.1996,\n",
      "          -0.0059, -0.9457,  0.1306]],\n",
      "\n",
      "        [[-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [ 0.2681,  2.2714,  0.4405, -0.6928,  0.4109, -0.4001,  0.6657,\n",
      "           1.6873,  0.1876, -0.3231],\n",
      "         [ 0.2837,  0.2744, -1.6337,  1.4281, -0.3537, -0.1521,  0.6761,\n",
      "          -0.5171, -0.7302,  0.6568],\n",
      "         [ 0.5543, -0.8159,  1.8696,  0.1121, -0.1792, -0.6431, -1.4316,\n",
      "          -1.0816,  0.9220,  2.0816],\n",
      "         [-0.4899,  0.1555, -1.7227,  0.4931,  0.4997,  0.5250, -0.3011,\n",
      "          -1.0627,  0.9369, -1.2027],\n",
      "         [ 0.6519,  1.5647, -0.4069,  1.6221, -0.4427,  0.3129, -0.0420,\n",
      "           0.4885,  1.3900,  0.1278]],\n",
      "\n",
      "        [[-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [ 0.2681,  2.2714,  0.4405, -0.6928,  0.4109, -0.4001,  0.6657,\n",
      "           1.6873,  0.1876, -0.3231],\n",
      "         [ 0.2837,  0.2744, -1.6337,  1.4281, -0.3537, -0.1521,  0.6761,\n",
      "          -0.5171, -0.7302,  0.6568],\n",
      "         [ 0.5543, -0.8159,  1.8696,  0.1121, -0.1792, -0.6431, -1.4316,\n",
      "          -1.0816,  0.9220,  2.0816],\n",
      "         [-0.4899,  0.1555, -1.7227,  0.4931,  0.4997,  0.5250, -0.3011,\n",
      "          -1.0627,  0.9369, -1.2027],\n",
      "         [ 0.6337,  1.1829,  1.7340, -0.1708, -0.3737,  1.5783,  1.7071,\n",
      "           0.1060, -0.5334, -0.8102]],\n",
      "\n",
      "        [[-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [ 1.4321, -0.3783,  0.4963, -0.6341, -0.8386,  0.4009,  3.1348,\n",
      "          -1.5262, -1.1780,  0.0658],\n",
      "         [-0.4542,  0.1029, -0.3196, -1.7591,  0.9019, -0.7763,  0.4036,\n",
      "           0.2324, -1.3783, -1.2706],\n",
      "         [ 2.1241,  0.6241,  0.4704,  0.0894,  0.3848,  0.7201, -0.7374,\n",
      "           0.4444,  1.5350,  0.3474],\n",
      "         [ 0.1842,  1.1685,  0.5233, -0.7669,  0.6694, -0.8667,  0.0460,\n",
      "           1.8485,  0.5150, -1.0578],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420]],\n",
      "\n",
      "        [[-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [-1.9109,  1.2603,  0.0902,  0.4615,  0.6784,  0.2136, -0.5116,\n",
      "          -0.0304, -0.3281, -0.4420],\n",
      "         [ 0.1329,  0.1161,  0.9492,  1.0956,  0.9329,  0.5660, -0.5574,\n",
      "          -0.3948,  0.8843, -0.0392],\n",
      "         [-0.4888, -0.6937,  1.0858, -1.6681,  2.1612, -1.9100,  0.7681,\n",
      "           0.0630, -1.5979,  1.7209],\n",
      "         [ 0.2286,  0.1535, -0.8487,  0.4032, -0.5673, -0.8090, -0.3270,\n",
      "           0.1259,  0.3009, -1.4169],\n",
      "         [-0.4899,  0.1555, -1.7227,  0.4931,  0.4997,  0.5250, -0.3011,\n",
      "          -1.0627,  0.9369, -1.2027]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import pad\n",
    "import numpy as np\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    'the glass of milk',\n",
    "    'the glass of juice',\n",
    "    'the cup of tea',\n",
    "    'I am a good boy',\n",
    "    'I am a good developer',\n",
    "    'understand the meaning of words',\n",
    "    'your videos are good'\n",
    "]\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = 10000\n",
    "\n",
    "# One-hot encoding function\n",
    "def one_hot_encoding(sentences, vocab_size):\n",
    "    word2index = {word: idx for idx, word in enumerate(set(\" \".join(sentences).split()))}\n",
    "    onehot_repr = [[word2index[word] % vocab_size for word in sentence.split()] for sentence in sentences]\n",
    "    return onehot_repr\n",
    "\n",
    "# One-hot encoding representation\n",
    "onehot_repr = one_hot_encoding(sentences, vocab_size)\n",
    "print(\"One-hot Encoded Representation:\", onehot_repr)\n",
    "\n",
    "# Pad sequences to ensure each sequence has the same length\n",
    "sentence_length = 8\n",
    "padded_docs = [np.pad(doc, (sentence_length - len(doc), 0), 'constant', constant_values=0) for doc in onehot_repr]\n",
    "padded_docs = torch.tensor(padded_docs)\n",
    "print(\"Padded Sequences:\\n\", padded_docs)\n",
    "\n",
    "# Embedding layer parameters\n",
    "embedding_dim = 10\n",
    "\n",
    "# Define PyTorch model with an Embedding layer\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, sentence_length):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.sentence_length = sentence_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        return embedded\n",
    "\n",
    "# Initialize and summarize the model\n",
    "model = EmbeddingModel(vocab_size, embedding_dim, sentence_length)\n",
    "print(model)\n",
    "\n",
    "# Generate embeddings\n",
    "embedded_output = model(padded_docs)\n",
    "print(\"Embedded Output:\\n\", embedded_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Snow",
   "language": "python",
   "name": "snow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
